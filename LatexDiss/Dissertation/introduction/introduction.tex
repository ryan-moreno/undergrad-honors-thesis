\addchapheadtotoc
\chapter{Introduction}
% answer:
% 1) What is the larger context (body of knowledge) for your topic?
% 2) What is the significance of your particular topic?
\section{Named Entity Recognition (NER)}
\subsection{Task Definition}
Named entity recognition (NER) is a fundamental information extraction task that attempts to extract entities from a given text and classify them using pre-defined categories (e.g. persons, locations, or organizations) \citep{2007Survey}.

\subsection{State-of-the-Art Frameworks}
Early NER models used hard-coded rules that specified the steps for extracting named entities \citep{NeuralNERSurvey}. Because of their dependence on pre-defined rules, these models were not robust. The next generation of models began to employ machine learning (notably, not deep learning) \citep{NeuralNERSurvey}. Although an improvement from rule-based approaches, feature engineering still requires researchers to pre-define which features are important to their NER model. Examples of such features include which words are capitalized in the sentence, the part of speech of each word, and the frequency of each word within the entire corpus \citep{2007Survey}.

Deep learning using neural networks offers an improvement on these previous approaches by abstracting away the definition of rules and features. Neural Networks attempt to model the power of the brain by representing neurons as linear threshold functions that act on a small piece of the input data. By connecting these neurons together, neural networks can represent non-linear functions that act on the entirety of the input data to produce an output prediction. Neural networks learn their non-linear function by repeatedly making predictions on the training data, verifying their predictions, and adjusting the weights of the neurons accordingly. In this way, neural networks learn to make predictions without hand-crafted rules or features; they simply need to be given a large set of training data with the correct outputs labeled. Since 2011, state-of-the-art NER models have employed neural networks, which outperform previous rule-based and feature-based approaches even without any access to external resources and domain-specific knowledge \citep{NeuralNERSurvey}.

The framework that we work with
Our Framework:
TODO: FEATURE ENG AND ELMO + GLoVE [read abt ELMO] and BiLSTM and attention contrastive loss and classification loss, CRF, CNN

\section{Problem Statement}
Neural NER models are powerful in settings in which an abundance of ground-truth training data is available \citep{LampleNER}. However, human annotation is expensive and time-consuming, especially in technical domains, such as biomedical publications, in which the human annotator must be able to understand and contextualize technical jargon. Unfortunately, neural NER model performance decreases significantly when training data is restricted \citep{TriggerNER}. Thus, a crucial research question is how we can most efficiently collect and use human-annotated ground-truth data. In this thesis, I investigate two potential solutions. First, I explore a novel way of collecting human-annotated data by gathering both ground-truth entity labels and \textit{entity triggers} -- the phrases which cue entities. This approach allows for state-of-the-art results using a smaller set of human annotations. Second, I explore automated methods to create additional training data without any additional human input.

\subsection{Entity Triggers}
\subsubsection{Our approach}
As the first method to more efficiently collect human annotations, I worked with USC's Intelligence and Knowledge Discovery Research Lab to introduce \textit{entity triggers} \citep{TriggerNER}. We hypothesize that collecting human explanations of the annotators' choices in entity labels could provide a more label-efficient learning of NER models. An entity trigger is a group of words in a sentence that helps to explain why a human would recognize an entity in the sentence. For example, in the sentence ``He ate lunch at IHOP,'' \textit{ate lunch at} is an trigger phrase for the entity \textit{IHOP}. After collecting the human annotations for both entities and entity triggers we use a neural network to learn to predict entity triggers in unlabeled sentences and used these predicted entity triggers as additional supervision for predicting entities. Our experiments demonstrate the cost effectiveness of using entity triggers. Training our framework on only 20\% of the trigger-annotated training sentences results in a comparable performance to conventional approaches using 70\% of the training data.

\subsubsection{Related Works}
% ~\citep{weaklabel} created automatically labeled NER data for a target language via annotation projection on comparable corpora. Then, they filter out weakly labeled (WL) sentences by statistical methods. However, this paper regards unlabeled words as 'O' so that it cannot deal with incomplete annotation. 
Towards low-resource learning for NER,
recent works have mainly focused on dictionary-based distantly supervision, using a dictionary of entities and an unannotated target domain corpus to generate weakly labeled data, data which we have less confidence in because it was not annotated by hand. This approach uses exact character matchings of words in the unannotated corpus with entities in the entity dictionary, regarding the hard-matched sentences as additional, weakly labeled data for learning a NER model. 
~\cite{autoner} and ~\cite{yangner} follow this approach, employing Partial CRFs to tentatively assign unlabeled words all possible labels, choosing one that maximizes the total probability. One drawback is that they rely on a domain-specific seed dictionary. 
~\cite{weakcrf} uses Wikipedia anchors, assumed to be named entities, as their entity dictionary. Using the Wikipedia anchors, they automatically construct weakly labeled data, splitting them into high-quality and noisy data. Then, they train a classification module, which regards name tagging as a multi-label classification problem. This module uses noisy data first and fine-tunes with high-quality data. After pre-training this classification module, they share the overall neural network with the sequence labeling module. Then, they use a sequence labeling module to infer the named entity. This paper does not rely on a domain-specific seed dictionary, but it relies on Wikipedia. One drawback is that their weak labeling method doesn't consider the context of a sentence.
~\cite{opencorrection} similarly constructs weakly labeled data from Wikipedia and DBpedia. Using a small amount of human annotated data, they implement a semi-supervised correction model with curriculum learning to correct the false-negative entity labels in the weakly labeled data. Finally, they use the corrected data to train a neural NER model. They show the effectiveness of curriculum learning using weakly labeled data, but it can only be applied to general NER tasks such as CONLL and ONTONOTES. It cannot be applied to domain-specific tasks.

Although the dictionary-based approaches largely reduce human efforts in annotating, the quality of matched sentences is highly dependent on the coverage of the dictionary and the quality of the corpus.
The learned models tend to have a bias towards entities with similar surface forms as the ones in dictionary. Without further tuning under better supervision, these models have low recall.
Unlike these works aiming to get rid of training data or human annotations, our work focuses on how to more cost-effectively utilize human efforts.

Another line of research which also aims to use human efforts more cost-effectively is active learning~\citep{shen2018deep,Lin2019AlpacaTagAA}.
This approach focuses on instance sampling and the human annotation UI, asking workers to annotate the most useful instances first.
However, a recent study~\citep{Lipton2018PracticalOT} argues that actively annotated data barely helps when training new models. 

Inspired by recent advances in learning sentence classification tasks using explanations or human-written rules~\citep{Li2018GeneralizeSK, Hancock2018TrainingCW, Wang2020Learning, Zhou2019NEROAN} such as relation extraction, we proposed the concept of an ``entity trigger'' for NER.
These prior works primarily focused on sentence classification, in which the rules are usually continuous token sequences and there is a single label for each input sentence.
The unique challenge in NER is that we have to deal with rules which are discontinuous token sequences and there may be multiple rules applied at the same time for an input instance. Our work using ``entity triggers'' sheds light on future research directions for more cost-effectively using human to learn NER models.




\subsection{Automated Data Generation}
\subsubsection{Adversarial Data}
An adversarial example for a neural network is an example that is created by perturbing a correctly classified example and causes the model to misclassify the  adversarial example. Adversarial examples are easily understood within the image domain, where a slight rearrangement in the pixels of an image results in a new image that is almost indistinguishable to a human (and thus would be classified the same as the original image). A classic example in pop-science is slightly rotating an image of a tabby cat that one of Google's image classifying networks was over 80\% confident was a cat, resulting in the network being over 90\% confident that the same image is a bowl of guacamole \citep{guacamole}. Adversarial examples in the natural language domain are slightly more complicated because rearranging words or characters in a sentence doesn't always result in a sentence that is syntactically valid, let alone semantically similar to the original sentence. I explore several ways to perturb sentences such that the original entity labels remain valid.

Adversarial examples are useful in part because they demonstrate the limits of state-of-the-art NER models. By creating adversarial data to effectively trick a NER model, we develop a better idea of the model's brittleness, which can guide future research directions for making the model more robust.

\subsubsection{Data Augmentation}
Another use of adversarial examples, the use that I focus on in this thesis, is data augmentation. By developing scripts to create adversarial examples, I've effectively developed scripts to take a set of labeled data and produce more, slightly different, labeled data. I hypothesize that since the adversarial examples of our test data are able to trick our NER models, the adversarial examples must have implicit knowledge that the NER model currently lacks, knowledge which could improve the NER model if it was embedded into the training data. With this in mind, I use the same scripts for creating adversarial examples to augment the training data, creating more, slightly different, training data without any extra human annotation cost. My experiments show that augmenting the training data improves the NER models' robustness to adversarial attacks while retaining its effectiveness on the original test data sets. TODO


\subsubsection{Related Works}
% SEARS:
% Complex machine learning models for NLP are often brittle, making different predic- tions for input instances that are extremely similar semantically. To automatically de- tect this behavior for individual instances, we present semantically equivalent ad- versaries (SEAs) – semantic-preserving perturbations that induce changes in the model’s predictions. We generalize these adversaries into semantically equivalent adversarial rules (SEARs) – simple, uni- versal replacement rules that induce ad- versaries on many instances. We demon- strate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual question- answering, and sentiment analysis. Via user studies, we demonstrate that we gener- ate high-quality local adversaries for more instances than humans, and that SEARs in- duce four times as many mistakes as the bugs discovered by human experts. SEARs are also actionable: retraining models us- ing data augmentation significantly reduces bugs, while maintaining accuracy.

%Truecaser:
% Although modern named entity recognition (NER) systems show impressive performance on standard datasets, they per- form poorly when presented with noisy data. In particular, capitalization is a strong signal for entities in many lan- guages, and even state of the art models overfit to this feature, with drastically lower performance on uncapitalized text. In this work, we address the problem of robustness of NER sys- tems in data with noisy or uncertain casing, using a pretrain- ing objective that predicts casing in text, or a truecaser, lever- aging unlabeled data. The pretrained truecaser is combined with a standard BiLSTM-CRF model for NER by appending output distributions to character embeddings. In experiments over several datasets of varying domain and casing quality, we show that our new model improves performance in un- cased text, even adding value to uncased BERT embeddings. Our method achieves a new state of the art on the WNUT17 shared task dataset.

%Alzentot:
% Deep neural networks (DNNs) are vulnera- ble to adversarial examples, perturbations to correctly classified examples which can cause the model to misclassify. In the image do- main, these perturbations are often virtually indistinguishable to human perception, caus- ing humans and state-of-the-art models to dis- agree. However, in the natural language do- main, small perturbations are clearly percep- tible, and the replacement of a single word can drastically alter the semantics of the doc- ument. Given these challenges, we use a black-box population-based optimization al- gorithm to generate semantically and syntac- tically similar adversarial examples that fool well-trained sentiment analysis and textual en- tailment models with success rates of 97% and 70%, respectively. 

%Jia:
% The num- ber of possible transformations scales expo- nentially with text length, so data augmenta- tion cannot cover all transformations of an in- put. This paper considers one exponentially large family of label-preserving transforma- tions, in which every word in the input can be replaced with a similar word. We train the first models that are provably robust to all word substitutions in this family. Our train- ing procedure uses Interval Bound Propaga- tion (IBP) to minimize an upper bound on the worst-case loss that any combination of word substitutions can induce. Our IBP-trained models attain 75% adversarial ac- curacy on both sentiment analysis on IMDB and natural language inference on SNLI. In comparison, on IMDB, models trained nor- mally and ones trained with data augmentation achieve adversarial accuracy of only 8% and 35%, respectively.

% Trigger paper:
% Recent advances in named entity recognition (NER) have shown that neural sequence tag- ging models are powerful in supervised learn- ing scenarios, where human-annotations are assumed to be abundant. However, the per- formance of such models dramatically drops when only a small amount of labels are avail- able. Thus, it is a crucial and practical research question that how we can efficiently learn a NER model with very limited human efforts.
% However, human annotation can be expensive and time-consuming in domain-specific NER prob- lems, such as biomedical publication, financial doc- uments, legal reports, etc. Unfortunately, powerful neural methods work significantly worse when only
% limited amount of entity labels are available. As we seek to advance NER to broader applications with less human efforts, label-efficiently learning NER models is thus a crucial research direction.


% Neural Survey paper:
% Starting with Collobert et al. (2011), neural network NER systems with minimal feature engineering have become popular. Such models are appealing because they typically do not require domain specific resources like lexicons or ontologies, and are thus poised to be more domain independent. 
% Our first finding from the survey is that feature-inferring NN systems outperform feature-engineered systems, despite the latter’s access to domain specific rules, knowledge, features, and lexicons. 
% Our next finding is that word+character hybrid models are generally better than both word-based and character-based models.
% NER systems are often used as the first step in question answering, information retrieval, co-reference resolution, topic modeling, etc
% Early NER systems were based on handcrafted rules, lexicons, orthographic fea- tures and ontologies. These systems were followed by NER systems based on feature-engineering and machine learning
% For example, NN models on news corpora improved the previous state-of-the-art by 1.59% in Spanish, 2.34% in German, 0.36% in English, and 0.14%, in Dutch, without any external resources or feature engineering.

% Feature Survey Paper:
% feature examples:  capitalization, POS, corpus frequency