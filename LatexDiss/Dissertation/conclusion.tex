\chapter{Conclusion}
% answer: What, briefly, did you learn or discover? What are the larger ramifications of your work?
In this thesis I present two approaches to increasing the efficiency of human annotations in named entity recognition tasks. First, I offer a novel technique for collecting human-annotated data by gathering both ground-truth entity labels and entity triggers. I present experiments which evidence the efficacy of entity trigger collection, showing that our framework trained on only 20\% of the available trigger-annotated training sentences results in a comparable performance to conventional approaches using 70\% of the training data. 

I also investigate a variety of methods to adversarially augment human-annotated data, creating new examples which retain the original labels, are semantically valid, and are difficult for existing models to label correctly. I experimentally demonstrate that the augmentation algorithms I developed are effective at fooling state-of-the-art NER models. Further, I provide evidence that adversarial training using these same augmentation techniques is effective at making models more robust to adversarial attacks without damaging the models' performance on standard data.

My work on adversarial augmentation is ongoing. I plan to further explore ways to improve my adversarial attacks. I also plan to test my augmentation algorithms on datasets other than CoNLL03. Finally, I would like to combine the topics of entity triggers and adversarial examples by adversarially attacking the trigger matching module and using adversarial training to make this module more robust.